diff --git a/projet_2/run_sb3.py b/projet_2/run_sb3.py
index 2e5f6c6..9bf1ebf 100644
--- a/projet_2/run_sb3.py
+++ b/projet_2/run_sb3.py
@@ -35,7 +35,7 @@ Check the documentation! https://stable-baselines3.readthedocs.io/en/master/
 import os
 from datetime import datetime
 # stable baselines 3
-from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
+from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecVideoRecorder
 from stable_baselines3 import PPO, SAC
 from stable_baselines3.common.env_util import make_vec_env
 # utils
@@ -46,12 +46,13 @@ from env.quadruped_gym_env import QuadrupedGymEnv
 
 import wandb
 from wandb.integration.sb3 import WandbCallback
+from stable_baselines3.common.monitor import Monitor
 
 
 
 LEARNING_ALG = "SAC" # or "SAC"
 LOAD_NN = False # if you want to initialize training with a previous model                   #HERE HERE HERE
-NUM_ENVS = 10    # how many pybullet environments to create for data collection
+NUM_ENVS = 1    # how many pybullet environments to create for data collection
 USE_GPU = True  # make sure to install all necessary drivers
 
 # after implementing, you will want to test how well the agent learns with your MDP: 
@@ -73,8 +74,8 @@ run = wandb.init(
     config=env_configs,
     sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics
     monitor_gym=True,  # auto-upload the videos of agents playing the game
-    save_code=True,
-    )# optional)
+    save_code=True,  # optional
+    )
 
 
 if USE_GPU and LEARNING_ALG=="SAC":
@@ -96,6 +97,8 @@ checkpoint_callback = CheckpointCallback(save_freq=20000//NUM_ENVS, save_path=SA
 # create Vectorized gym environment
 env = lambda: QuadrupedGymEnv(**env_configs)  
 env = make_vec_env(env, monitor_dir=SAVE_PATH, n_envs=NUM_ENVS)
+#env = VecVideoRecorder(env, f"videos/{run.id}", record_video_trigger=lambda x: x % 2000 == 0, video_length=200)
+
 # normalize observations to stabilize learning (why?)
 
 
@@ -105,7 +108,7 @@ else:
     env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=100.)
 
 # Multi-layer perceptron (MLP) policy of two layers of size _,_ 
-policy_kwargs = dict(net_arch=[256,256])
+policy_kwargs = dict(net_arch=[256, 256])
 # What are these hyperparameters? Check here: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html
 n_steps = 4096 
 learning_rate = lambda f: 1e-4 
@@ -121,7 +124,7 @@ ppo_config = {  "gamma":0.99,
                 "clip_range":0.2, 
                 "clip_range_vf":1,
                 "verbose":1, 
-                "tensorboard_log":f"runs/{run.id}",
+                "tensorboard_log":f"runs",
                 "_init_setup_model":True, 
                 "policy_kwargs":policy_kwargs,
                 "device": gpu_arg}
@@ -137,15 +140,17 @@ sac_config={"learning_rate":1e-4,
             "gradient_steps":1,
             "learning_starts": 10000,
             "verbose":1, 
-            "tensorboard_log":f"runs/{run.id}",
+            "tensorboard_log":f"runs",
             "policy_kwargs": policy_kwargs,
             "seed":None, 
             "device": gpu_arg}
 
 if LEARNING_ALG == "PPO":
     model = PPO('MlpPolicy', env, **ppo_config)
+    #wandb.config.update(ppo_config)
 elif LEARNING_ALG == "SAC":
     model = SAC('MlpPolicy', env, **sac_config)
+    #wandb.config.update(sac_config)
 else:
     raise ValueError(LEARNING_ALG + 'not implemented')
 
@@ -157,11 +162,11 @@ if LOAD_NN:
     print("\nLoaded model", model_name, "\n")
 
 # Learn and save (may need to train for longer)
-model.learn(total_timesteps=1000000, log_interval=1, callback=WandbCallback(gradient_save_freq=20000//NUM_ENVS, model_save_path=SAVE_PATH, verbose=2))
+model.learn(total_timesteps=1000000, callback=WandbCallback())
 # Don't forget to save the VecNormalize statistics when saving the agent
 model.save(os.path.join(SAVE_PATH, "rl_model") )
 env.save(os.path.join(SAVE_PATH, "vec_normalize.pkl"))
-if LEARNING_ALG == "SAC": # save replay buffer 
+if LEARNING_ALG == "SAC": # save replay buffer
     model.save_replay_buffer(os.path.join(SAVE_PATH, "off_policy_replay_buffer"))
 run.finish()
 
diff --git a/projet_2/runs/337u62h2/SAC_1/events.out.tfevents.1670334799.LAPTOP-GUI.35668.0 b/projet_2/runs/337u62h2/SAC_1/events.out.tfevents.1670334799.LAPTOP-GUI.35668.0
deleted file mode 100644
index 5b31feb..0000000
Binary files a/projet_2/runs/337u62h2/SAC_1/events.out.tfevents.1670334799.LAPTOP-GUI.35668.0 and /dev/null differ
diff --git a/projet_2/testemp.py b/projet_2/testemp.py
new file mode 100644
index 0000000..6c8f123
--- /dev/null
+++ b/projet_2/testemp.py
@@ -0,0 +1,45 @@
+import gym
+from stable_baselines3 import PPO
+from stable_baselines3.common.monitor import Monitor
+from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder
+import wandb
+from wandb.integration.sb3 import WandbCallback
+
+
+
+
+config = {
+    "policy_type": "MlpPolicy",
+    "total_timesteps": 25000,
+    "env_name": "CartPole-v1",
+}
+run = wandb.init(
+    project="sb3",
+    config=config,
+    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics
+    monitor_gym=True,  # auto-upload the videos of agents playing the game
+    save_code=True,  # optional
+)
+
+
+
+
+def make_env():
+    env = gym.make(config["env_name"])
+    env = Monitor(env)  # record stats such as returns
+    return env
+
+
+
+
+env = DummyVecEnv([make_env])
+env = VecVideoRecorder(env, "videos", record_video_trigger=lambda x: x % 2000 == 0, video_length=200)
+model = PPO(config["policy_type"], env, verbose=1, n_steps=512, tensorboard_log=f"runs")
+model.learn(
+    total_timesteps=config["total_timesteps"],
+    callback=WandbCallback(
+        gradient_save_freq=100,
+        model_save_path=f"models/{run.id}",
+        verbose=2,
+    ),
+)
